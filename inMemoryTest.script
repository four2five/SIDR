#!/bin/bash

$HADOOP_HOME/bin/hadoop dfs -rmr /netcdf_output

# $HADOOP_HOME/bin/hadoop jar $SCIHADOOP_HOME/build/jar/hadoop-scidata-v2.jar netcdf_hdfs_filter -D damasc.extraction_shape=1,4276737 -D damasc.partition_mode=record -D damasc.placement_mode=sampling -D damasc.query_dependant=false -D damasc.number_reducers=1 -D damasc.variable_name=vals_nod_var1 -D damasc.buffer_size=4194304 -D damasc.logfile=/tmp/damasc_log.txt -D damasc.scihadoop=true -D damasc.reducer.dynamic_start=false -D damasc.fs_type=hdfs -D damasc.coordinate_variable_name=coordx /user/ceph-admin/paul_input/file1.nc /user/ceph-admin/foo 

# without dependency scheduling
$HADOOP_HOME/bin/hadoop jar $SCIHADOOP_HOME/build/jar/$SCIHADOOP_JAR netcdf_hdfs_median -D damasc.extraction_shape=2,36,36,10 -D damasc.partition_mode=record -D damasc.placement_mode=sampling -D damasc.query_dependant=false -D damasc.number_reducers=9 -D damasc.variable_name=windspeed1 -D damasc.buffer_size=4194304 -D damasc.logfile=/tmp/damasc_log.txt -D damasc.scihadoop=true -D damasc.reducer.dynamic_start=false -D damasc.use_combiner=false -D damasc.sample_ratio=0.0001 -D damasc.partitioner_class=arrayspec /netcdf_input/file1.nc /netcdf_output

# WITH dependency scheduling
#$HADOOP_HOME/bin/hadoop jar $SCIHADOOP_HOME/build/jar/$SCIHADOOP_HAR netcdf_hdfs_median -D damasc.extraction_shape=2,36,36,10 -D damasc.partition_mode=record -D damasc.placement_mode=sampling -D damasc.query_dependant=false -D damasc.number_reducers=1 -D damasc.variable_name=windspeed1 -D damasc.buffer_size=4194304 -D damasc.logfile=/tmp/damasc_log.txt -D damasc.scihadoop=true -D damasc.reducer.dynamic_start=true -D damasc.sample_ratio=0.0001 -D damasc.partitioner_class=arrayspec /netcdf_input/file1.nc /netcdf_output



