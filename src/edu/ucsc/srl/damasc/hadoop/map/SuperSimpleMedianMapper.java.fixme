package edu.ucsc.srl.damasc.hadoop.map;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Arrays;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.TaskAttemptID;
import org.apache.hadoop.mapreduce.TaskID;

import edu.ucsc.srl.damasc.hadoop.Utils;
import edu.ucsc.srl.damasc.hadoop.io.ArraySpec;
import edu.ucsc.srl.damasc.hadoop.io.DataIterator;
import edu.ucsc.srl.damasc.hadoop.io.HolisticResult;
//import java.lang.Thread;
//import org.apache.hadoop.io.IntWritable;
//import edu.ucsc.srl.damasc.hadoop.io.HolisticResult2;

/*
import ucar.ma2.Array;
import ucar.ma2.ArrayInt;
*/

/**
 * Dummy mapper, just passed data through with a dummy key.
 * This is used for testing purposes
 */
public class SuperSimpleMedianMapper extends Mapper<ArraySpec, ByteBuffer, ArraySpec, IntWritable> {

  private static int DATATYPESIZE = 4;

  @SuppressWarnings("unused")
  private static final Log LOG = LogFactory.getLog(SuperSimpleMedianMapper.class);

 /**
 * Reduces values for a given key
 * @param key ArraySpec representing the given Array being passed in
 * @param value an Array to process that corresponds to the given key 
 * @param context the Context object for the currently executing job
 */
  public void map(ArraySpec key, ByteBuffer inArray, Context context)
                  throws IOException, InterruptedException {

    TaskAttemptID attempt = context.getTaskAttemptID();
    TaskID task = attempt.getTaskID();

    try {

      long timer = System.currentTimeMillis();
     
      long elementCount = Utils.calcTotalSize(key.getShape());
      System.out.println("Array Spec has " + elementCount + " elements");

      int[] extractionShape = Utils.getExtractionShape(context.getConfiguration(),
                                                        key.getShape().length);
      int extShapeSize = Utils.calcTotalSize(extractionShape);

      int[] allOnes = new int[extractionShape.length];
      for( int i=0; i<allOnes.length; i++){
        allOnes[i] = 1;
      }

      ArraySpec arraySpec = new ArraySpec(key.getCorner(), "");

      System.out.println("in mapper, corner is: " + 
                         Utils.arrayToString(key.getCorner()) + 
                         " shape: " + Utils.arrayToString(key.getShape())
                         + " extsize: " + extShapeSize + 
                         " extShape: " + Arrays.toString(extractionShape));

      
      DataIterator dataItr = new DataIterator(inArray, key.getCorner(),
                                                  key.getShape(), extractionShape,
                                                  DATATYPESIZE);
      int[] tempGroup;
      int[] tempArray = new int[extractionShape.length];
      long totalElements = 0;
      //int medianValue = 0;
      //int totalCount = 0;
      

      // test that the size of the ByteBuffer matches the expected size
      assert( inArray.capacity()/DATATYPESIZE == elementCount);

      // should only need to set ths once
      arraySpec.setVariable(key.getVarName());

      IntWritable intW = new IntWritable();
      
      while( dataItr.hasMoreGroups() ) { 
        tempGroup = dataItr.getNextGroup();


        while( dataItr.groupHasMoreValues() ) { 
          totalElements++;
          // write out every element individually
          intW.set(dataItr.getNextValueInt());
          Utils.mapToLocal(tempGroup, tempArray, arraySpec, extractionShape);
          context.write(arraySpec, intW, 1);
        }

      }

      timer = System.currentTimeMillis() - timer;

      System.out.println("Just wrote " + totalElements + " for map task " + 
                         task.getId() + 
                         " and it took " + timer + " ms"); 

    } catch ( Exception e ) {
      System.out.println("Caught an exception in SuperSimpleMedianMapper.map()" + e.toString() );
      e.printStackTrace();
    }
  }
}
