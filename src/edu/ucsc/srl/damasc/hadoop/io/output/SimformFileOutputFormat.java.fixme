package edu.ucsc.srl.damasc.hadoop.io.output;

import java.io.DataOutputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.ArrayList;
import java.util.HashMap;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.permission.FsPermission;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.SequenceFile.CompressionType;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.typedbytes.TypedBytesWritable;

import ucar.ma2.Array;
import ucar.ma2.DataType;
import ucar.ma2.InvalidRangeException;
import ucar.nc2.Dimension;
import ucar.nc2.NetcdfFileWriteable;
import ucar.nc2.Variable;

import edu.ucsc.srl.damasc.hadoop.Utils;
import edu.ucsc.srl.damasc.hadoop.io.ArraySpec;
import edu.ucsc.srl.damasc.hadoop.io.CoordVariable;
//import edu.ucsc.srl.damasc.hadoop.io.NetCDFTools;

import edu.ucsc.srl.damasc.hadoop.HadoopUtils;

public class SimformFileOutputFormat<K, V> extends FileOutputFormat<K, V> { 

  private static final Log LOG = 
      LogFactory.getLog(SimformFileOutputFormat.class);

  protected static class SimformWriter<K, V> extends RecordWriter<K,V> { 
	  private Configuration _conf;
    //private DataOutputStream _out;
    private SequenceFile.Writer _fileWriter = null;
    private SequenceFile.Writer _indexWriter = null;
	  private ByteBuffer _outBuffer;
	  private int _dataTypeSize;
    private String _lastFileName;
    private FileSystem _fs = null;
    private Path _defaultDir = null;
    private Path _dcacheFile = null;
    
	  public SimformWriter(TaskAttemptContext context) 
	    throws IOException { 
	
	    //this._ncwFile = ncwFile;
      //this._out = out;
	    this._conf = context.getConfiguration();
	    this._dataTypeSize = Utils.getDataTypeSize(context.getOutputValueClass()); 
      this._lastFileName = "";
      this._fs = FileSystem.get(this._conf);
      this._defaultDir = context.getWorkingDirectory(); 
      LOG.info("Output datatype is " + this._dataTypeSize + " bytes");


      //this._indexWriter = SequenceFile.createWriter(_fs, _conf, fileToCreate, 
       //                                             TypedBytesWritable, TypedBytesWritable,
        //                                            CompressionType.RECORD);
      try { 
        Path[] cacheFiles = DistributedCache.getLocalCacheFiles(_conf);
        if (null != cacheFiles && cacheFiles.length > 0) { 
          for (Path cacheFile : cacheFiles) { 
            LOG.info("cache file: " + cacheFile.toString());
            if (cacheFile.toString().endsWith(this._conf.get(Utils.CACHED_COORD_FILE_NAME))) { 
              LOG.info("\t**** Found the damasc cache file: " + cacheFile.toString());
              this._dcacheFile = cacheFile;
              break;
            }
          }
        } else { 
          LOG.info("There are no cache files dummy");
        }
	    } catch (IOException ioe) { 
        LOG.info("IOException in SimformFileOutputFormat");
        LOG.info(ioe.toString());
      }
    }

	  public synchronized void close(TaskAttemptContext context) throws IOException { 
      // close the last file we were writing to, if it exists
      if (null != this._fileWriter) { 
	      this._fileWriter.close();
      }
	  } 

    // close out the last file (if it existed) and open up a new file
    private void createNewInputFile(String inputFileName) { 
      if (null != this._fileWriter) { 
        this._fileWriter.close();
        this._fileWriter = null;
      }

      //LOG.info("in SimformFileOutputFormat, creating file: " + fileToRead);
      //int reducerID = job.getTaskAttemptID().getTaskID().getId();
      // -jbuck
      Path perFileDir = new Path(this._defaultDir, inputFileName.split(".")[0]);
      LOG.info("Per file dir is " + perFileDir.toString());
      boolean retVal =  this._fs.mkdirs(perFileDir, new FsPermission("755"));
      LOG.info("mkdirs retVal: " + retVal);

      // create the index file
      Path indexFile = new Path(this._defaultDir, "index.seq");
      Path indexFileToCreate = Utils.convertToMountPath(indexFile, _conf);
      LOG.info("in SimformFileOutputFormat, creating index file: " + indexFileToCreate);
      this._indexWriter = SequenceFile.createWriter(this._fs, _conf, indexFileToCreate, 
                                                    TypedBytesWritable.class, TypedBytesWritable.class,
                                                    CompressionType.RECORD);

      Path file = new Path(this._defaultDir, inputFileName + "_part0.seq");
      Path fileToCreate = Utils.convertToMountPath(file, _conf);
      LOG.info("in SimformFileOutputFormat, creating file: " + fileToCreate);
      this._fileWriter = SequenceFile.createWriter(this._fs, _conf, fileToCreate, 
                                                    TypedBytesWritable.class, TypedBytesWritable.class,
                                                    CompressionType.RECORD);

      // now write out the coordinate variables to the new XXX_part0.seq file
      writeCoordValues();
    }

    private void writeCoordValues() { 
      // first, open up the cache file and read the number of variables in it
      FSDataInputStream inStream = _fs.open(this._dcacheFile);
      int numVars = inStream.readInt();
      LOG.info("file " + this._dcacheFile.toString() + " has " + numVars + " vars");

      // stash variables in a HashMap until they're all loaded
      CoordVariable tmpCV;
      HashMap<String, float[]> cvMap = new HashMap<String, float[]>(numVars);
      ByteBuffer tmpBB;
      float[] tmpFloatBuffer;


      for (int i=0; i<numVars; i++) { 
        tmpCV = new CoordVariable();
        tmpCV.readFields(inStream);
        LOG.info("just loaded cv: " + tmpCV.getVarName() + " with " + 
                 tmpCV.getData().length + " bytes");
        // convert CoordVars BytesWritable to double[] (simform specific code here)
        //tmpBB = (ByteBuffer.wrap(tmpCV.getData())).asDoubleBuffer().array();
        tmpBB = ByteBuffer.wrap(tmpCV.getData());
        int floatCapacity = tmpBB.capacity() / (Double.SIZE / 8);
        LOG.info("  this var has " + floatCapacity + " floats in it");
        tmpFloatBuffer = new float[floatCapacity];

        // now walk through the entries, one at a time, and convert the 
        // doubles to floats (mimicing simform behavior)
        int j=0;
        double testDouble;
        while (tmpBB.hasRemaining()) { 
          testDouble = tmpBB.getDouble();
          tmpFloatBuffer[j] = (float)(testDouble);
          if ( j % 1024 == 0) { 
            LOG.info("\t\tj: " + j + " d: " + testDouble + " f: " + tmpFloatBuffer[j]);
          }
          j++;
        }
        LOG.info(" converted" + j + " doubles to floats");
        cvMap.put(tmpCV.getVarName(), tmpFloatBuffer);
        LOG.info(" added " + tmpCV.getVarName() + " to map");
      }

      // close the DistributedCache file as we're done reading from it
      inStream.close(); 

      // now write out the variables in the prescribed order
      TypedBytesWritable key = new TypedBytesWritable();
      TypedBytesWritable value = new TypedBytesWritable();

      // coordx
      if (!cvMap.containsKey("coordx")) { 
        LOG.info("cvMap does NOT contain coordX. This is very bad");
      } else { 
        key.setValue(new Integer(-1));
        value.setValue(cvMap.get("coordx"));
        this._fileWriter.append(key,value);
      }

      // coordy
      if (!cvMap.containsKey("coordy")) { 
        LOG.info("cvMap does NOT contain coordy. This is very bad");
      } else { 
        key.setValue(new Integer(-1));
        value.setValue(cvMap.get("coordy"));
        this._fileWriter.append(key,value);
      }

      // coordz
      if (!cvMap.containsKey("coordz")) { 
        LOG.info("cvMap does NOT contain coordz. This is very bad");
      } else { 
        key.setValue(new Integer(-1));
        value.setValue(cvMap.get("coordz"));
        this._fileWriter.append(key,value);
      }

      // done writing out the coord vars for this file
    }
	
	  public synchronized void write( K key, V value) throws IOException { 
      ArraySpec k = (ArraySpec)key;

      // create a new file if we're onto data from a different input file
      if (k.getFileName() != _lastFileName) { 
        createNewInputFile(k.getFileName());
      }

      //this._fileWriter.append(key, value);

	  }
  }

  public RecordWriter<K, V> getRecordWriter(TaskAttemptContext job) 
    throws IOException, InterruptedException { 

    Configuration conf = job.getConfiguration();
    Path path = getDefaultWorkFile(job, ""); 
    LOG.warn("Default work file: " + path.toString());
    //FileSystem fs = NetCDFTools.getFS(conf);
    //Path intendedFile = FileOutputFormat.getTaskOutputPath(job
    //FileSystem fs = FileSystem.get(conf);
    //int reducerID = job.getTaskAttemptID().getTaskID().getId();
    //Path fileToRead = Utils.convertToMountPath(path, conf);
    //LOG.info("in SimformFileOutputFormat, creating file: " + fileToRead);

    //LOG.debug("path: " + fileToRead.toString());
    //LOG.debug("parent: " + fileToRead.getParent().toString());
    //LOG.debug("file: " + fileToRead.getName().toString());

    // create intervening directories
    /*
    boolean retVal =  _fs.mkdirs(path.getParent(), new FsPermission("777"));
    LOG.info("mkdirs retVal: " + retVal);

    FileStatus[] files = _fs.listStatus(path.getParent());

    if (null == files) { 
      LOG.warn("path: " + path.toString() + " is not a dir / returned null");
    } else { 
      LOG.warn("Dir: " + path.getParent().toString() + " has files: " );
      for (FileStatus file : files ) { 
        LOG.warn("\t[ " + file.getPath().toString() + " ]");
      }
    }

    //stat the directory to ensure that it exists
    FileStatus statedDir = this._fs.getFileStatus(path.getParent());

    if (statedDir.isDir()) { 
      LOG.warn("path: " + statedDir.getPath().toString() + " is a dir"); 
    } else { 
      LOG.warn("path: " + statedDir.getPath().toString() + " is NOT a dir"); 
    }

    // create the file in the file system. This will create any missing directories
    // in the path
    //LOG.warn("creating file: " + fileToRead.toString());
    //FSDataOutputStream fileOut = fs.create(fileToRead);

    // we create the files in the FileOutputFormat since we'll likely be needing 
    // to create more than one per Reducer instance
    */
    SimformWriter<K, V> retWriter = new SimformWriter<K, V>(job);

    return retWriter;
  }

}
