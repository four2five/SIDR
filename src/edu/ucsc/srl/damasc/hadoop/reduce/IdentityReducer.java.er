package edu.ucsc.srl.damasc.hadoop.reduce;

import java.io.IOException;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.TaskAttemptID;
import org.apache.hadoop.mapreduce.TaskID;

import edu.ucsc.srl.damasc.hadoop.HadoopUtils;
import edu.ucsc.srl.damasc.hadoop.Utils;
import edu.ucsc.srl.damasc.hadoop.io.ArraySpec;
//import edu.ucsc.srl.damasc.hadoop.io.IntArrayWritable;
//import edu.ucsc.srl.damasc.hadoop.HDF5Utils;

/**
 * Reducer that simply iterates through the data it is passed
 */

/*
 public class IdentityReducer
  extends Reducer<GroupID, IntArrayWritable, GroupID, IntWritable> { 

  public void reduce( GroupID gid, Iterable<IntArrayWritable> values, Context context) 
    throws IOException, InterruptedException { 

    IntArrayWritable tempIAW;
    IntWritable[] tempIW;

    for( IntArrayWritable val : values) { 
      tempIAW = val; 
      tempIW = (IntWritable[])tempIAW.get();
      for( int i=0; i<tempIW.length; i++) { 
        context.write(gid, tempIW[i], 1); 
      }
    }
  }
*/
public class IdentityReducer extends 
        Reducer<ArraySpec, IntWritable, ArraySpec, IntWritable> {

  @SuppressWarnings("unused")
private static final Log LOG = LogFactory.getLog(IdentityReducer.class);

  public void setup(Context context) throws IOException, InterruptedException { 
    super.setup(context);

    //int taskID = Utils.parseTaskID( context.getTaskAttemptID().toString()); 
    //System.out.println("in reducer.setup(), calling getMapTasksForReducer");
    TaskAttemptID attempt = context.getTaskAttemptID();
    TaskID task = attempt.getTaskID();

    System.out.println("in reduce.setup for task: " + task.getId());

    int[] outputCornerForThisReducer = 
      HadoopUtils.getOutputCornerForReducerN(task.getId(), context.getConfiguration() );

    int numReducers = Utils.getNumberReducers(context.getConfiguration());

    int[] outputShapeForThisReducer = 
      HadoopUtils.getReducerWriteShape( task.getId(), context.getConfiguration());

    int[] totalGlobalOutputSpace = Utils.getTotalOutputSpace(context.getConfiguration());

    if( task.getId() == (numReducers - 1) )
      outputShapeForThisReducer = Utils.correctArray(outputCornerForThisReducer, 
                                                     outputShapeForThisReducer, 
                                                     context.getConfiguration());

    System.out.println("Reducer: " + task.getId() + " of " + numReducers + 
                    " write corner: " +  Utils.arrayToString(outputCornerForThisReducer) + 
                    " shape: " + Utils.arrayToString(outputShapeForThisReducer) + 
                    " totalOutputSpace: " + Utils.arrayToString(totalGlobalOutputSpace));
    //int[] mapTasks = HDF5Utils.getMapTasksForReducer(taskID, context.getConfiguration());
  }
  public void reduce(ArraySpec key, Iterable<IntWritable> values, 
                     Context context)
                     throws IOException, InterruptedException {

    //System.out.println("in reducer, key: " + key.toString() );
    long timer = System.currentTimeMillis();

    // debug test
    //LongWritable maxVal = new LongWritable();
    //maxVal.set(Long.MIN_VALUE);

    ArraySpec prevKey = new ArraySpec();
    // empty loop
    for (IntWritable value : values) {

      if( key.getCorner().length < 4) { 
        System.out.println("key.getGroupID().length < 4. It's " + key.getCorner().length);
        System.out.println("prevGroupID: " + Utils.arrayToString(prevKey.getCorner()));
      }

      context.write(key, value, (long)1);
      prevKey.setCorner(key.getCorner());
    }


    timer = System.currentTimeMillis() - timer;
    //LOG.info("total reducer took: " + timer + " ms");
  }
}
