package edu.ucsc.srl.damasc.hadoop.reduce;

import java.io.IOException;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapreduce.Reducer;

import edu.ucsc.srl.damasc.hadoop.io.ArraySpec;
import edu.ucsc.srl.damasc.hadoop.io.IntArrayWritable;
//import org.apache.hadoop.mapreduce.TaskAttemptID;
//import org.apache.hadoop.mapreduce.TaskID;
//import org.apache.hadoop.io.IntWritable;


//import edu.ucsc.srl.damasc.hadoop.HDF5Utils;
//import edu.ucsc.srl.damasc.hadoop.Utils;
//import edu.ucsc.srl.damasc.hadoop.HadoopUtils;
//import edu.ucsc.srl.damasc.hadoop.io.GroupID;

/**
 * Reducer that simply iterates through the data it is passed
 */

 public class FilterReducer
  extends Reducer<ArraySpec, IntArrayWritable, ArraySpec, Text> { 

  public void reduce( ArraySpec as, Iterable<IntArrayWritable> values, Context context) 
    throws IOException, InterruptedException { 

    /*
    ArrayList<V> tempArrayList = new ArrayList<V>();

    for( V val : values) { 
      tempArrayList.add(val);
    }
    */

    IntArrayWritable tempIAW;
    Writable[] tempIW;
    String outString = "";
    Text textOut = new Text();

    for( IntArrayWritable val : values) { 
      tempIAW = val; 
      tempIW = tempIAW.get();
      for( int i=0; i<tempIW.length; i++) { 

        /*
        textOut.set(tempIW[i].toString());
        context.write(gid, textOut, 1); 
        */

        if( "" != outString ) { 
          outString += "," + tempIW[i].toString();
        } else { 
          outString += tempIW[i].toString();
        }

      }
    }
    if ("" != outString) { 
      textOut.set(outString);
      context.write(as, textOut, 1); 
    }
  }
}
